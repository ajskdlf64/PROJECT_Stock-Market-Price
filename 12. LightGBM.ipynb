{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**관련 논문**\n",
    "https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf\n",
    "\n",
    "\n",
    "**번역본**\n",
    "https://aldente0630.github.io/data-science/2018/06/29/highly-efficient-gbdt.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **max_depth** : 나무의 깊이. 단일 결정나무에서는 충분히 데이터를 고려하기 위해 depth를 적당한 깊이로 만들지만, 부스팅에서는 깊이 하나짜리도 만드는 등, 깊이가 짧은것이 크리티컬하지 않다. 어차피 보정되니까..\n",
    "\n",
    "\n",
    " - **min_data_in_leaf** : 한 잎사귀 노드에 들어갈수 있는 데이터의 갯수.\n",
    "\n",
    "\n",
    " - **feature_fraction** : 부스팅 대상 모델이 랜덤포레스트일때, 랜덤포레스트는 feature의 일부만을 선택하여 훈련하는데, 이를 통제하기 위한 파라미터.\n",
    "\n",
    "\n",
    " - **bagging_fraction** : 데이터의 일부만을 사용하는 bagging의 비율이다. 예를들어 오버피팅을 방지하기 위해 데이터의 일부만을 가져와서 훈련시키는데, 이는 오버피팅을 방지하며 약한예측기를 죄다 합칠경우는 오히려 예측성능이 좋아질것이다.\n",
    "\n",
    "\n",
    " - **early_stopping_round** : 더이상 validation데이터에서 정확도가 좋아지지 않으면 멈춰버린다. 훈련데이터는 거의 에러율이 0에 가깝게 좋아지기 마련인데, validation데이터는 훈련에 사용되지 않기때문에 일정이상 좋아지지 않기 때문이다. 이후 진행하게 되면 컴퓨터 재능낭비다.\n",
    "\n",
    "\n",
    " - **lambda** : 정규화에 사용되는 파라미터\n",
    "\n",
    "\n",
    " - **min_gain_to_split** : 최소 정보이득이 있어야 분기가 되게끔 만든다.\n",
    "\n",
    "\n",
    " - **max_cat_group** : 범주형 변수가 많으면, 하나로 퉁쳐서 처리하게끔 만드는 최소단위.\n",
    "\n",
    "\n",
    " - **objective** : lightgbm은 regression, binary, multicalss 모두 가능\n",
    "\n",
    "\n",
    " - **boosting** : gbdt(gradient boosting decision tree), rf(random forest), dart(dropouts meet multiple additive regression trees), goss(Gradient-based One-Side Sampling)\n",
    "\n",
    "\n",
    " - **num_leaves** : 결정나무에 있을 수 있는 최대 잎사귀 수. 기본값은 0.31\n",
    "\n",
    "\n",
    " - **learning_rate** : 상당히 중요한 파라미터인데, 각 예측기마다 얼마나 가중치를 주어 학습하게 할것인지 만든다. learning_rate은 아래의 num_boost_round와 잘 맞춰져야 ㅎ나다.\n",
    "\n",
    "\n",
    " - **num_boost_round** : boosting을 얼마나 돌릴지 지정한다. 경험상 보통 100정도면 너무 빠르게 끝나며, 시험용이 아니면 1000정도는 준다. 어차피 early_stopping_round가 지정되어있으면 더이상 진전이 없을경우 알아서 멈춘다.\n",
    "\n",
    "\n",
    " - **device** : gpu, cpu\n",
    "\n",
    "\n",
    " - **metric** : loss를 측정하기 위한 기준. mae (mean absolute error), mse (mean squared error), 등등이 있다. 사실 엄청많다.\n",
    "\n",
    "\n",
    " - **max_bin** : 최대 bin\n",
    "\n",
    "\n",
    " - **categorical_feature** : 범주형 변수가 있을때 여기에 언급해준다. 다만, categorical은 string은 쓸수 없는것으로 보인다.\n",
    "\n",
    "\n",
    " - **ignore_column** : 컬럼을 무시한다. 무시하지 않을경우 모두 training에 넣는데, 뭔가 남겨놓아야할 컬럼이 있으면 여기다가 인자를 준다.\n",
    "\n",
    "\n",
    " - **save_binary** : True로 해놓으면 메모리를 아낀다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
